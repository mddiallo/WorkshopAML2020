{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoML Forecasting with Azure ML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='https://github.com/retkowsky/images/blob/master/AzureMLservicebanniere.png?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This python notebook uses Azure ML service and its AUTOML features to automatically generate a time series model.\n",
    "We are using a time series dataset with daily observations.\n",
    "> https://docs.microsoft.com/en-us/python/api/overview/azure/ml/intro?view=azure-ml-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Version Python 3.6 du notebook\n",
    "import sys\n",
    "print(\"Version Python:\",sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "print(\"Date :\" , datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Paramétrage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "from pandas.tseries.frequencies import to_offset\n",
    "\n",
    "# Squash warning messages for cleaner output in the notebook\n",
    "warnings.showwarning = lambda *args, **kwargs: None\n",
    "\n",
    "from azureml.core.workspace import Workspace\n",
    "from azureml.core.experiment import Experiment\n",
    "from azureml.train.automl import AutoMLConfig\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As part of the setup you have already created a <b>Workspace</b>. To run AutoML, you also need to create an <b>Experiment</b>. An Experiment corresponds to a prediction problem you are trying to solve, while a Run corresponds to a specific approach to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "print(\"Version Azure ML service :\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()\n",
    "\n",
    "# choose a name for the run history container in the workspace\n",
    "experiment_name = 'Exemple4-automlforecast'\n",
    "# project folder\n",
    "project_folder = './sample_projects/workshop4'\n",
    "\n",
    "experiment = Experiment(ws, experiment_name)\n",
    "\n",
    "output = {}\n",
    "output['SDK version'] = azureml.core.VERSION\n",
    "output['Workspace'] = ws.name\n",
    "output['Resource Group'] = ws.resource_group\n",
    "output['Location'] = ws.location\n",
    "output['Project Directory'] = project_folder\n",
    "output['Run History Name'] = experiment_name\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "outputDf = pd.DataFrame(data = output, index = [''])\n",
    "outputDf.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydateparser = lambda x: pd.datetime.strptime(x, \"%d/%m/%Y\")\n",
    "data = pd.read_csv(\"Pollution.csv\" , encoding = \"iso-8859-1\", parse_dates = ['Date'] , \n",
    "date_parser = mydateparser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Lignes, colonnes :\",data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrice de corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['CO2']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('CO2')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Temperature']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Température')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['O3_Ozone']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Ozone')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Vent']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Vent')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Humidite']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Humidité')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['SO2_Dioxyde_de_soufre']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Dioxyde de soufre')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['Direction_vent']].plot(figsize=(20,8), linewidth=2, fontsize=10)\n",
    "plt.title('Direction du vent')\n",
    "plt.xlabel('Date', fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AutoML Forecasting\n",
    "We are going to predict CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_column_name = 'CO2' #Target variable to predict\n",
    "time_column_name = 'Date'  #Date variable\n",
    "grain_column_names = []    #Optional. Needed for multiple time series ie location..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/616f2d9f-eb6a-4f3b-af56-cc82197db001.png\" height=\"50\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Split the data\n",
    "\n",
    "The first split we make is into train and test sets. Note we are splitting on time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[data[time_column_name] < '2019-10-01']\n",
    "test = data[data[time_column_name] >= '2019-10-01']\n",
    "\n",
    "X_train = train.copy()\n",
    "y_train = X_train.pop(target_column_name).values\n",
    "\n",
    "X_test = test.copy()\n",
    "y_test = X_test.pop(target_column_name).values\n",
    "\n",
    "print(\"Training dimensions:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print()\n",
    "print(\"Test dimensions:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Setting forecast maximum horizon \n",
    "\n",
    "The forecast horizon is the number of periods into the future that the model should predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_horizon = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Azure AutoML specification\n",
    "\n",
    "Instantiate a AutoMLConfig object. This defines the settings and data used to run the experiment.\n",
    "\n",
    "|Property|Description|\n",
    "|-|-|\n",
    "|**task**|forecasting|\n",
    "|**primary_metric**|This is the metric that you want to optimize.<br> Forecasting supports the following primary metrics <br><i>spearman_correlation</i><br><i>normalized_root_mean_squared_error</i><br><i>r2_score</i><br><i>normalized_mean_absolute_error</i>\n",
    "|**iterations**|Number of iterations. In each iteration, Auto ML trains a specific pipeline on the given data|\n",
    "|**iteration_timeout_minutes**|Time limit in minutes for each iteration.|\n",
    "|**X**|(sparse) array-like, shape = [n_samples, n_features]|\n",
    "|**y**|(sparse) array-like, shape = [n_samples, ], targets values.|\n",
    "|**n_cross_validations**|Number of cross validation splits.|\n",
    "|**country_or_region**|The country/region used to generate holiday features. These should be ISO 3166 two-letter country/region codes (i.e. 'US', 'GB').|\n",
    "|**path**|Relative path to the project folder.  AutoML stores configuration files for the experiment under this folder. You can specify a new empty folder. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Documentation : https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl_settings = {\n",
    "    'time_column_name': time_column_name,\n",
    "    'max_horizon': max_horizon,\n",
    "    'target_lags': 10,\n",
    "    \n",
    "}\n",
    "\n",
    "automl_config = AutoMLConfig(task='forecasting',\n",
    "                             debug_log = 'automl4.log',\n",
    "                             primary_metric='normalized_root_mean_squared_error',\n",
    "                             iterations=5,\n",
    "                             iteration_timeout_minutes = 5,\n",
    "                             experiment_timeout_minutes = 15,\n",
    "                             enable_early_stopping=True,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             n_cross_validations=5,  \n",
    "                             enable_voting_ensemble=False,\n",
    "                             enable_stack_ensemble=False,\n",
    "                             path=project_folder,\n",
    "                             **automl_settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Normalized root-mean-square deviation. This value is commonly referred to as the normalized root-mean-square deviation or error ( NRMSD or NRMSE ), and often expressed as a percentage, where lower values indicate less residual variance. https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-configure-auto-train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_run = experiment.submit(automl_config, show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link to Azure Portal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Retrieve the Best Model\n",
    "Below we select the best pipeline from our iterations. The **get_output** method on automl_classifier returns the best run and the fitted model for the last fit invocation. There are overloads on get_output that allow you to retrieve the best run and fitted model for any logged metric or a particular iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 View the engineered names for featurized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 View the featurization summary\n",
    "\n",
    "You can also see what featurization steps were performed on different raw features in the user data. For each raw feature in the user data, the following information is displayed:\n",
    "\n",
    "- Raw feature name\n",
    "- Number of engineered features formed out of this raw feature\n",
    "- Type detected\n",
    "- If feature was dropped\n",
    "- List of feature transformations for the raw feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the featurization summary as a list of JSON\n",
    "featurization_summary = fitted_model.named_steps['timeseriestransformer'].get_featurization_summary()\n",
    "# View the featurization summary as a pandas dataframe\n",
    "pd.DataFrame.from_records(featurization_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the best fitted model from the AutoML Run to make forecasts for the **test set.**  \n",
    "\n",
    "We always score on the original dataset whose schema matches the training set schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_outputs(y_predicted, X_trans, X_test, y_test, predicted_column_name='predicted',\n",
    "                  horizon_colname='horizon_origin'):\n",
    "    \"\"\"\n",
    "    Demonstrates how to get the output aligned to the inputs\n",
    "    using pandas indexes. Helps understand what happened if\n",
    "    the output's shape differs from the input shape, or if\n",
    "    the data got re-sorted by time and grain during forecasting.\n",
    "    \n",
    "    Typical causes of misalignment are:\n",
    "    * we predicted some periods that were missing in actuals -> drop from eval\n",
    "    * model was asked to predict past max_horizon -> increase max horizon\n",
    "    * data at start of X_test was needed for lags -> provide previous periods\n",
    "    \"\"\"\n",
    "    df_fcst = pd.DataFrame({predicted_column_name : y_predicted,\n",
    "                            horizon_colname: X_trans[horizon_colname]})\n",
    "    # y and X outputs are aligned by forecast() function contract\n",
    "    df_fcst.index = X_trans.index\n",
    "    \n",
    "    # align original X_test to y_test    \n",
    "    X_test_full = X_test.copy()\n",
    "    X_test_full[target_column_name] = y_test\n",
    "\n",
    "    # X_test_full's index does not include origin, so reset for merge\n",
    "    df_fcst.reset_index(inplace=True)\n",
    "    X_test_full = X_test_full.reset_index().drop(columns='index')\n",
    "    together = df_fcst.merge(X_test_full, how='right')\n",
    "    \n",
    "    # drop rows where prediction or actuals are nan \n",
    "    # happens because of missing actuals \n",
    "    # or at edges of time due to lags/rolling windows\n",
    "    clean = together[together[[target_column_name, predicted_column_name]].notnull().all(axis=1)]\n",
    "    return(clean)\n",
    "\n",
    "def do_rolling_forecast(fitted_model, X_test, y_test, max_horizon, freq='D'):\n",
    "    \"\"\"\n",
    "    Produce forecasts on a rolling origin over the given test set.\n",
    "    \n",
    "    Each iteration makes a forecast for the next 'max_horizon' periods \n",
    "    with respect to the current origin, then advances the origin by the horizon time duration. \n",
    "    The prediction context for each forecast is set so that the forecaster uses \n",
    "    the actual target values prior to the current origin time for constructing lag features.\n",
    "    \n",
    "    This function returns a concatenated DataFrame of rolling forecasts.\n",
    "     \"\"\"\n",
    "    df_list = []\n",
    "    origin_time = X_test[time_column_name].min()\n",
    "    while origin_time <= X_test[time_column_name].max():\n",
    "        # Set the horizon time - end date of the forecast\n",
    "        horizon_time = origin_time + max_horizon * to_offset(freq)\n",
    "        \n",
    "        # Extract test data from an expanding window up-to the horizon \n",
    "        expand_wind = (X_test[time_column_name] < horizon_time)\n",
    "        X_test_expand = X_test[expand_wind]\n",
    "        y_query_expand = np.zeros(len(X_test_expand)).astype(np.float)\n",
    "        y_query_expand.fill(np.NaN)\n",
    "        \n",
    "        if origin_time != X_test[time_column_name].min():\n",
    "            # Set the context by including actuals up-to the origin time\n",
    "            test_context_expand_wind = (X_test[time_column_name] < origin_time)\n",
    "            context_expand_wind = (X_test_expand[time_column_name] < origin_time)\n",
    "            y_query_expand[context_expand_wind] = y_test[test_context_expand_wind]\n",
    "        \n",
    "        # Make a forecast out to the maximum horizon\n",
    "        y_fcst, X_trans = fitted_model.forecast(X_test_expand, y_query_expand)\n",
    "        \n",
    "        # Align forecast with test set for dates within the current rolling window \n",
    "        trans_tindex = X_trans.index.get_level_values(time_column_name)\n",
    "        trans_roll_wind = (trans_tindex >= origin_time) & (trans_tindex < horizon_time)\n",
    "        test_roll_wind = expand_wind & (X_test[time_column_name] >= origin_time)\n",
    "        df_list.append(align_outputs(y_fcst[trans_roll_wind], X_trans[trans_roll_wind],\n",
    "                                     X_test[test_roll_wind], y_test[test_roll_wind]))\n",
    "        \n",
    "        # Advance the origin time\n",
    "        origin_time = horizon_time\n",
    "    \n",
    "    return pd.concat(df_list, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's do the forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = do_rolling_forecast(fitted_model, X_test, y_test, max_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We now calculate some error metrics for the forecasts and vizualize the predictions vs. the actuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def APE(actual, pred):\n",
    "    \"\"\"\n",
    "    Calculate absolute percentage error.\n",
    "    Returns a vector of APE values with same length as actual/pred.\n",
    "    \"\"\"\n",
    "    return 100*np.abs((actual - pred)/actual)\n",
    "\n",
    "def MAPE(actual, pred):\n",
    "    \"\"\"\n",
    "    Calculate mean absolute percentage error.\n",
    "    Remove NA and values where actual is close to zero\n",
    "    \"\"\"\n",
    "    not_na = ~(np.isnan(actual) | np.isnan(pred))\n",
    "    not_zero = ~np.isclose(actual, 0.0)\n",
    "    actual_safe = actual[not_na & not_zero]\n",
    "    pred_safe = pred[not_na & not_zero]\n",
    "    return np.mean(APE(actual_safe, pred_safe))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple forecasting model\")\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.3f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.3f' % mae)\n",
    "print('MAPE: %.3f' % MAPE(df_all[target_column_name], df_all['predicted']))\n",
    "\n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Model Prediction vs Actual')\n",
    "test_pred = plt.scatter(df_all[target_column_name], df_all['predicted'], color='green')\n",
    "test_test = plt.scatter(y_test, y_test, color='blue')\n",
    "plt.legend((test_pred, test_test), ('Model prediction', 'Actual'), loc='upper left', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.groupby('horizon_origin').apply(\n",
    "    lambda df: pd.Series({'MAPE': MAPE(df[target_column_name], df['predicted']),\n",
    "                          'RMSE': np.sqrt(mean_squared_error(df[target_column_name], df['predicted'])),\n",
    "                          'MAE': mean_absolute_error(df[target_column_name], df['predicted'])}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_APE = df_all.assign(APE=APE(df_all[target_column_name], df_all['predicted']))\n",
    "APEs = [df_all_APE[df_all['horizon_origin'] == h].APE.values for h in range(1, max_horizon + 1)]\n",
    "\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.boxplot(APEs)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('horizon')\n",
    "plt.ylabel('APE (%)')\n",
    "plt.title('Absolute Percentage Errors by Forecast Horizon')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple forecasting model\")\n",
    "rmse = np.sqrt(mean_squared_error(df_all[target_column_name], df_all['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.3f\" % rmse)\n",
    "mae = mean_absolute_error(df_all[target_column_name], df_all['predicted'])\n",
    "print('mean_absolute_error score: %.3f' % mae)\n",
    "print('MAPE: %.3f' % MAPE(df_all[target_column_name], df_all['predicted']))\n",
    "\n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 8))\n",
    "pred, = plt.plot(df_all[time_column_name], df_all['predicted'], color='green')\n",
    "actual, = plt.plot(df_all[time_column_name], df_all[target_column_name], color='blue')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.legend((pred, actual), ('Model Prediction', 'Actual'), loc='upper left', fontsize=12)\n",
    "plt.title('Prediction vs. Actual Time-Series')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "df_all.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. AutoML Forecasting with lags\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/80748202-8fef-4d74-b8b1-bdd27e6838aa.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lags parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/6690a165-f602-4c2e-ba5d-24437505ef69.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Rolling Windows Size parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://azurecomcdn.azureedge.net/mediahandler/acomblog/media/Default/blog/d03d6941-a7b8-4023-a4ab-d8f56751bfe4.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_settings_with_lags = {\n",
    "    'time_column_name': time_column_name,\n",
    "    'max_horizon': max_horizon,\n",
    "    'country_or_region': 'FR',\n",
    "    'target_lags': 10,\n",
    "    'target_rolling_window_size': 30\n",
    "}\n",
    "\n",
    "automl_config_lags = AutoMLConfig(task='forecasting',\n",
    "                                  debug_log='automlwork42.log',\n",
    "                                  primary_metric='normalized_root_mean_squared_error',\n",
    "                                  iterations=3,\n",
    "                                  iteration_timeout_minutes=10,\n",
    "                                  enable_voting_ensemble=False,\n",
    "                                  enable_stack_ensemble=False,\n",
    "                                  X=X_train,\n",
    "                                  y=y_train,\n",
    "                                  n_cross_validations=3,\n",
    "                                  path=project_folder,\n",
    "                                  verbosity=logging.INFO,\n",
    "                                  **time_series_settings_with_lags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "local_run_lags = experiment.submit(automl_config_lags, show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_run_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "RunDetails(local_run_lags).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "children = list(local_run_lags.get_children())\n",
    "metricslist = {}\n",
    "for run in children:\n",
    "    properties = run.get_properties()\n",
    "    metrics = {k: v for k, v in run.get_metrics().items() if isinstance(v, float)}\n",
    "    metricslist[int(properties['iteration'])] = metrics\n",
    "\n",
    "rundata = pd.DataFrame(metricslist).sort_index(1)\n",
    "rundata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run_lags.get_output()\n",
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_model.named_steps['timeseriestransformer'].get_engineered_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2 = do_rolling_forecast(fitted_model, X_test, y_test, max_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple forecasting model\")\n",
    "rmse = np.sqrt(mean_squared_error(df_all2[target_column_name], df_all2['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.3f\" % rmse)\n",
    "mae = mean_absolute_error(df_all2[target_column_name], df_all2['predicted'])\n",
    "print('mean_absolute_error score: %.3f' % mae)\n",
    "print('MAPE: %.3f' % MAPE(df_all2[target_column_name], df_all2['predicted']))\n",
    "\n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.title('Absolute Percentage Errors by Forecast Horizon')\n",
    "test_pred = plt.scatter(df_all2[target_column_name], df_all2['predicted'], color='green')\n",
    "test_test = plt.scatter(y_test, y_test, color='blue')\n",
    "plt.legend((test_pred, test_test), ('Model Prediction', 'Actual'), loc='upper left', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2.groupby('horizon_origin').apply(\n",
    "    lambda df: pd.Series({'MAPE': MAPE(df[target_column_name], df['predicted']),\n",
    "                          'RMSE': np.sqrt(mean_squared_error(df[target_column_name], df['predicted'])),\n",
    "                          'MAE': mean_absolute_error(df[target_column_name], df['predicted'])}))\n",
    "\n",
    "\n",
    "df_all2_APE = df_all2.assign(APE=APE(df_all2[target_column_name], df_all2['predicted']))\n",
    "APEs = [df_all2_APE[df_all2['horizon_origin'] == h].APE.values for h in range(1, max_horizon + 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple forecasting model\")\n",
    "rmse = np.sqrt(mean_squared_error(df_all2[target_column_name], df_all2['predicted']))\n",
    "print(\"[Test Data] \\nRoot Mean squared error: %.3f\" % rmse)\n",
    "mae = mean_absolute_error(df_all2[target_column_name], df_all2['predicted'])\n",
    "print('mean_absolute_error score: %.3f' % mae)\n",
    "print('MAPE: %.3f' % MAPE(df_all2[target_column_name], df_all2['predicted']))\n",
    "\n",
    "# Plot outputs\n",
    "%matplotlib inline\n",
    "plt.figure(figsize=(20, 8))\n",
    "pred, = plt.plot(df_all2[time_column_name], df_all2['predicted'], color='green')\n",
    "actual, = plt.plot(df_all2[time_column_name], df_all2[target_column_name], color='blue')\n",
    "plt.xticks(fontsize=8)\n",
    "plt.legend((pred, actual), ('Model Prediction', 'Actual'), loc='upper left', fontsize=12)\n",
    "plt.title('Prediction vs. Actual Time-Series')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all2_APE.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Get started with time-series forecasting in automated ML.** With these new capabilities automated ML increases support more complex forecasting scenarios, provides more control to configure training data using lags and window aggregation and improves accuracy with new holiday featurization and ROCV. Azure Machine Learning aims to enable data scientists of all skill levels to use powerful machine learning technology that simplifies their processes and reduces the time spent training models. Get started by visiting our documentation and let us know what you think - we are committed to make automated ML better for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run, fitted_model = local_run.get_output()\n",
    "print(best_run)\n",
    "print()\n",
    "print(fitted_model)\n",
    "print()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "for metric_name in best_run_metrics:\n",
    "    metric = best_run_metrics[metric_name]\n",
    "    print(metric_name, metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Model\n",
    "\n",
    "best_run.register_model(model_path='outputs/model.pkl', model_name='Exemple4-AutoML-Forecast',\n",
    "                        tags={'Training context':'Azure Auto ML'},\n",
    "                        properties={'R2': best_run_metrics['r2_score'], 'RMSE': best_run_metrics['normalized_root_mean_squared_error']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des modèles référencés\n",
    "for model in Model.list(ws):\n",
    "    print(model.name, 'version =', model.version)\n",
    "    for tag_name in model.tags:\n",
    "        tag = model.tags[tag_name]\n",
    "        print ('\\t',tag_name, ':', tag)\n",
    "    for prop_name in model.properties:\n",
    "        prop = model.properties[prop_name]\n",
    "        print ('\\t',prop_name, ':', prop)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Documentation : https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-automated-ml<br>\n",
    "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-configure-auto-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/retkowsky/images/blob/master/Powered-by-MS-Azure-logo-v2.png?raw=true\" height=\"300\" width=\"300\">"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "erwright"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3.6 - AzureML",
   "language": "python",
   "name": "python3-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
